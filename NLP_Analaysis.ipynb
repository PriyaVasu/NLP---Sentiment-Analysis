{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-rqw3YDGXMv"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "from gensim import corpora, models, similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D5ojPwLGXM2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.stem import SnowballStemmer\n",
        "df = pd.read_csv('Training_Full_V1.1.csv', error_bad_lines=False)\n",
        "def clean_text(text):\n",
        "\n",
        "    ## Remove puncuation\n",
        "    text = text.translate(string.punctuation)\n",
        "\n",
        "    ## Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "\n",
        "    ## Remove stop words\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
        "\n",
        "    text = \" \".join(text)\n",
        "    ## Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    text = re.sub(r\"<|;\", \" \", text)\n",
        "    ## Stemming\n",
        "    text = text.split()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stemmed_words = [stemmer.stem(word) for word in text]\n",
        "    text = \" \".join(stemmed_words)\n",
        "    return text\n",
        "# apply the above function to df['text']\n",
        "df['sentence'] = df['sentence'].map(lambda x: clean_text(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL9pHXcWGXM4"
      },
      "outputs": [],
      "source": [
        "tok_corp= [nltk.word_tokenize(sent) for sent in df['sentence']]\n",
        "model = gensim.models.Word2Vec(tok_corp, min_count=1, size = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1JiCsV2GXM5",
        "outputId": "01a66143-dde6-4ac1-cf87-4bcd7568a8e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "184"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = []\n",
        "max_len = 0\n",
        "for i in range(len(df['sentence'])):\n",
        "    if max_len<len(df['sentence'][i].split(\" \")):\n",
        "        max_len = len(df['sentence'][i].split(\" \"))\n",
        "max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GEJDeSwGXM7",
        "outputId": "ae5e0976-b044-4a36-c444-c023c1f4a63e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ksrao/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ]
        }
      ],
      "source": [
        "train = []\n",
        "blank = [0]*100\n",
        "for i in range(len(df['sentence'])):\n",
        "    sent_prob = []\n",
        "    sent = df['sentence'][i].split(\" \")\n",
        "    if len(sent) >50:\n",
        "        for k in range(50):\n",
        "            prob=[]\n",
        "            word_prob = model.wv.most_similar(sent[k],topn=100)\n",
        "            for j in range(len(word_prob)):\n",
        "                prob.append(word_prob[j][1])\n",
        "            sent_prob.append(prob)\n",
        "    else:\n",
        "        for k in range(len(sent)):\n",
        "            prob=[]\n",
        "            word_prob = model.wv.most_similar(sent[k],topn=100)\n",
        "            for j in range(len(word_prob)):\n",
        "                prob.append(word_prob[j][1])\n",
        "            sent_prob.append(prob)\n",
        "\n",
        "        for k in range(50-len(sent)):\n",
        "            sent_prob.append(blank)\n",
        "    train.append(sent_prob)\n",
        "    if i == 3440:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QASzuxlkGXM8"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "for l in df['label'][0:3441]:\n",
        "    labels.append(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cOaIPrxGXM9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwVts59jGXM_"
      },
      "outputs": [],
      "source": [
        "X,y = shuffle(train,labels,random_state=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,labels, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb1ADGBlGXNA"
      },
      "outputs": [],
      "source": [
        "def flat_func(data):\n",
        "    data1 = []\n",
        "    for i in range(len(data)):\n",
        "        data1.append(data[i].flatten())\n",
        "    return data1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1h1r_k7GXNB"
      },
      "outputs": [],
      "source": [
        "def onehotcode(labels):\n",
        "    y_true = []\n",
        "    for i in range(len(labels)):\n",
        "        list1 = np.zeros(2)\n",
        "        list1[labels[i]] = 1\n",
        "        y_true.append(list1)\n",
        "    return y_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKCuZRuBGXNB"
      },
      "outputs": [],
      "source": [
        "x_train = flat_func(np.array(X_train))\n",
        "x_test = flat_func(np.array(X_test))\n",
        "y_train_label = onehotcode(y_train)\n",
        "y_test_label  = onehotcode(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSCkD2FHGXNC"
      },
      "outputs": [],
      "source": [
        "len(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy75h8aOGXNC"
      },
      "outputs": [],
      "source": [
        "def init_weights(shape):\n",
        "    init_random_dist = tf.truncated_normal(shape,stddev=0.1)\n",
        "    return tf.Variable(init_random_dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqsVkuOxGXNC"
      },
      "outputs": [],
      "source": [
        "def init_bias(shape):\n",
        "    init_bias_vals = tf.constant(0.1,shape=shape)\n",
        "    return tf.Variable(init_bias_vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJPZEqTDGXND"
      },
      "outputs": [],
      "source": [
        "def conv2d(x,W):\n",
        "    # x----> [batch,H,W,Channels]\n",
        "    # W----> [filter H, filter W , Channels IN, Channels OUT]\n",
        "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tokecdI0GXND"
      },
      "outputs": [],
      "source": [
        "def max_pool_2by2(x):\n",
        "    # x---> [batch,H,W,Channels]\n",
        "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUTWQsjwGXND"
      },
      "outputs": [],
      "source": [
        "# Convolutional Layer\n",
        "def convolutional_layer(input_x,shape):\n",
        "    W = init_weights(shape)\n",
        "    b = init_bias([shape[3]])\n",
        "    return tf.nn.relu(conv2d(input_x,W)+b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBwtLV0JGXND"
      },
      "outputs": [],
      "source": [
        "# Normal (fully connected)\n",
        "def normal_full_layer(input_layer,size):\n",
        "    input_size = int(input_layer.get_shape()[1])\n",
        "    W = init_weights([input_size,size])\n",
        "    b = init_bias([size])\n",
        "    return tf.matmul(input_layer,W)+b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4jNdc5zGXNE"
      },
      "outputs": [],
      "source": [
        "x = tf.placeholder(tf.float32,shape=[None,5000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_ZZBHL6GXNE"
      },
      "outputs": [],
      "source": [
        "y_true = tf.placeholder(tf.float32,shape=[None,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndQqtaftGXNE"
      },
      "outputs": [],
      "source": [
        "x_image = tf.reshape(x,[-1,50,100,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBR3oaCqGXNE"
      },
      "outputs": [],
      "source": [
        "convo_1 = convolutional_layer(x_image,shape=[5,5,1,32])\n",
        "convo_1_pooling = max_pool_2by2(convo_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WYNxpx5GXNE"
      },
      "outputs": [],
      "source": [
        "convo_2_flat = tf.reshape(convo_1_pooling,[-1,25*50*32])\n",
        "full_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat,1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcdYeuRFGXNF"
      },
      "outputs": [],
      "source": [
        "#Dropout\n",
        "hold_prob = tf.placeholder(tf.float32)\n",
        "full_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tVCfE9jGXNF"
      },
      "outputs": [],
      "source": [
        "y_pred = normal_full_layer(full_one_dropout,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP4K5gIMGXNF"
      },
      "outputs": [],
      "source": [
        "#Lost Function\n",
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true,logits=y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-Su3bzzGXNF"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "train = optimizer.minimize(cross_entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgIZPbJ6GXNF"
      },
      "outputs": [],
      "source": [
        "init = tf.global_variables_initializer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y1pJ6JLGXNG"
      },
      "outputs": [],
      "source": [
        "saver = tf.train.Saver()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhHmE4EZGXNG"
      },
      "outputs": [],
      "source": [
        "steps = 599\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    j=0\n",
        "    k=0\n",
        "    for i in range(1,steps):\n",
        "        j = j+8\n",
        "        batch_x,batch_y = x_train[k*8:j],y_train_label[k*8:j]\n",
        "        k = k+1\n",
        "        sess.run(train,feed_dict={x:batch_x,y_true:batch_y,hold_prob:0.5})\n",
        "        #3conv_result= (sess.run(convo_1,feed_dict={x:batch_x}))\n",
        "        #conv_pooling_result= (sess.run(convo_1_pooling,feed_dict={x:batch_x}))\n",
        "        if i%299==0:\n",
        "            j=0\n",
        "            k=0\n",
        "\n",
        "        if i%100 == 0:\n",
        "            print(\"ON STEP:{}\".format(i),end=' ')\n",
        "            print(\"------------>\",end=' ')\n",
        "            print(\"ACCURACY: \",end=' ')\n",
        "            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n",
        "            acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
        "            print(sess.run(acc,feed_dict={x:x_test,y_true:y_test_label,hold_prob:1.0}))\n",
        "    saver.save(sess,'digit_model/weights_v2.ckpt')\n",
        "    #result = sess.run(y_pred,feed_dict={x:[data],hold_prob:1.0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gURcrlxRGXNH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}